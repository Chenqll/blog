# FineTune for downstream task
[Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377) (MAE) 是计算机视觉领域的一种自监督训练方式，使得模型能够在没有标签的数据上进行预训练，然后在少量的有标签数据上进行微调就可以获得媲美甚至超过有监督预训练方式的性能。本指南将向您展示如何在在 [Stanford Cars](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) 数据集上微调以 MAE 的方式在 ImageNet 数据集上预训练得到的 ViT 模型。
## load
## preposses
## train